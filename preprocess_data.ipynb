{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Local imports\n",
    "from utils import scroll_df, calculate_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview operational training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_operational_df = pd.read_csv(\"raw_data/train_operational_readouts.csv\")\n",
    "print(train_operational_df.shape)\n",
    "train_operational_sorted_df = train_operational_df.sort_values(by='vehicle_id')\n",
    "train_operational_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_df(((train_operational_df.isna().sum() / len(train_operational_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview time to event training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tte_df = pd.read_csv(\"raw_data/train_tte.csv\")\n",
    "print(train_tte_df.shape)\n",
    "train_tte_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_df(((train_tte_df.isna().sum() / len(train_tte_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of healthy vehicles and repaired one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_and_repaired_vehicles = train_tte_df[\"in_study_repair\"].value_counts()\n",
    "\n",
    "labels = [\"Healthy\", \"Repaired\"]\n",
    "colors = [\"skyblue\", \"lightgreen\", \"salmon\"]\n",
    "\n",
    "def autopct_format(pct, all_vals):\n",
    "    absolute = int(round(pct / 100. * sum(all_vals)))  # Calculate count\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(healthy_and_repaired_vehicles, \n",
    "        labels=healthy_and_repaired_vehicles.index, \n",
    "        autopct=lambda pct: autopct_format(pct, healthy_and_repaired_vehicles),\n",
    "        startangle=180, \n",
    "        colors=colors)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(labels, title=\"Vehicle Status\", loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Category Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge opertational and time to event dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_df = pd.merge(train_operational_df, train_tte_df, on='vehicle_id', how='inner')\n",
    "print(combined_train_df.shape)\n",
    "combined_train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate class label, remove timesteps for healthy 48 time steps close to length_of_study_time_step value and plot class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_df[\"class\"] = combined_train_df.apply(lambda row: calculate_class(row[\"in_study_repair\"], row[\"time_step\"], row[\"length_of_study_time_step\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_df = combined_train_df[\n",
    "    ~((combined_train_df[\"in_study_repair\"] == 0) & \n",
    "      ((combined_train_df[\"length_of_study_time_step\"] - combined_train_df[\"time_step\"]) < 48))\n",
    "]\n",
    "\n",
    "combined_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of class occurrences sequence wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of the sequence labels and calculate percentages\n",
    "combined_train_df = combined_train_df.sort_values(by=[\"vehicle_id\", \"time_step\"]).reset_index(drop=True)\n",
    "# Obtains the last class variable for each vehicles sequence\n",
    "last_class_df = combined_train_df.groupby(\"vehicle_id\")[\"class\"].last().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = last_class_df.value_counts()\n",
    "class_percentages = (class_counts / class_counts.sum()) * 100\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(class_counts.index, class_percentages.values, color=[\"skyblue\", \"lightgreen\", \"salmon\", \"orange\", \"purple\"])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Distribution of Classes (in % and Count)\")\n",
    "plt.xticks([0, 1, 2, 3, 4])  # Ensure correct class labels\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)  # Add grid for better readability\n",
    "\n",
    "# Show percentage and count on bars\n",
    "for i, (count, pct) in enumerate(zip(class_counts.values, class_percentages.values)):\n",
    "    plt.text(class_counts.index[i], pct + 1, f\"{pct:.1f}% ({count})\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebalance the data for the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out a number of healthy vehicles and remove them to downsample the majority class 0\n",
    "healthy_vehicles = train_tte_df.query(\"in_study_repair == 0\")[\"vehicle_id\"]\n",
    "print(\"total healthy vehciles\", healthy_vehicles.shape)\n",
    "total_healthy_vehicles = 21278\n",
    "healthy_vehicles_to_keep = 2272\n",
    "healthy_vehicles_to_be_removed = healthy_vehicles.sample(n=total_healthy_vehicles - healthy_vehicles_to_keep, random_state=50)\n",
    "print(\"healthy_vehicles_to_be_removed\", healthy_vehicles_to_be_removed.shape)\n",
    "\n",
    "rebalanced_train_tte_df = train_tte_df[~train_tte_df[\"vehicle_id\"].isin(healthy_vehicles_to_be_removed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalanced_healthy_and_repaired_vehicles = rebalanced_train_tte_df[\"in_study_repair\"].value_counts()\n",
    "\n",
    "labels = [\"Healthy\", \"Repaired\"]\n",
    "colors = [\"skyblue\", \"lightgreen\", \"salmon\"]\n",
    "\n",
    "def autopct_format(pct, all_vals):\n",
    "    absolute = int(round(pct / 100. * sum(all_vals)))  # Calculate count\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(rebalanced_healthy_and_repaired_vehicles, \n",
    "        labels=rebalanced_healthy_and_repaired_vehicles.index, \n",
    "        autopct=lambda pct: autopct_format(pct, rebalanced_healthy_and_repaired_vehicles),\n",
    "        startangle=180, \n",
    "        colors=colors)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(labels, title=\"Vehicle Status\", loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Category Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalanced_df = combined_train_df[~combined_train_df[\"vehicle_id\"].isin(healthy_vehicles_to_be_removed)]\n",
    "\n",
    "# Count occurrences of the sequence labels and calculate percentages\n",
    "rebalanced_df = rebalanced_df.sort_values(by=[\"vehicle_id\", \"time_step\"]).reset_index(drop=True)\n",
    "# Obtains the last class variable for each vehicles sequence\n",
    "rebalanced_last_class_df = rebalanced_df.groupby(\"vehicle_id\")[\"class\"].last().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences and calculate percentages\n",
    "rebalanced_class_counts = rebalanced_last_class_df.value_counts()\n",
    "rebalanced_class_percentages = (rebalanced_class_counts / rebalanced_class_counts.sum()) * 100\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(rebalanced_class_counts.index, rebalanced_class_percentages.values, color=[\"skyblue\", \"lightgreen\", \"salmon\", \"orange\", \"purple\"])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Distribution of Classes (in % and Count)\")\n",
    "plt.xticks([0, 1, 2, 3, 4])  # Ensure correct class labels\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)  # Add grid for better readability\n",
    "\n",
    "# Show percentage and count on bars\n",
    "for i, (count, pct) in enumerate(zip(rebalanced_class_counts.values, rebalanced_class_percentages.values)):\n",
    "    plt.text(rebalanced_class_counts.index[i], pct + 1, f\"{pct:.1f}% ({count})\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train/validation/test 80/10/10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview the df columns \n",
    "scroll_df(rebalanced_df.head(60))\n",
    "# Sort the df on timestep to obtain the correct sequence to have the class correct class at end\n",
    "\n",
    "last_class_df = rebalanced_df.groupby(\"vehicle_id\")[\"class\"].last().reset_index()\n",
    "\n",
    "# Rename column for clarity\n",
    "print(last_class_df[\"class\"].value_counts())\n",
    "\n",
    "# Stratify the split based on last_class\n",
    "train_ids, tmp_ids = train_test_split(last_class_df, test_size=0.2, stratify=last_class_df[\"class\"], random_state=50)\n",
    "print(\"Train: \", train_ids.shape)\n",
    "print(train_ids[\"class\"].value_counts())\n",
    "val_ids, test_ids = train_test_split(tmp_ids, test_size=0.5, stratify=tmp_ids[\"class\"], random_state=50)\n",
    "\n",
    "print(\"Val: \", val_ids.shape)\n",
    "print(val_ids[\"class\"].value_counts())\n",
    "\n",
    "print(\"Test: \", test_ids.shape)\n",
    "print(test_ids[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick out the correct vehicle ids for the different set from the main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vehicle_ids for train, val, and test sets\n",
    "train_vehicle_ids = train_ids[\"vehicle_id\"]\n",
    "val_vehicle_ids = val_ids[\"vehicle_id\"]\n",
    "test_vehicle_ids = test_ids[\"vehicle_id\"]\n",
    "\n",
    "train_df = rebalanced_df[rebalanced_df[\"vehicle_id\"].isin(train_vehicle_ids)].reset_index(drop=True)\n",
    "val_df = rebalanced_df[rebalanced_df[\"vehicle_id\"].isin(val_vehicle_ids)].reset_index(drop=True)\n",
    "test_df = rebalanced_df[rebalanced_df[\"vehicle_id\"].isin(test_vehicle_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that drop index worked!\n",
    "scroll_df(val_df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation strategies to create two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before imputing remove the columns that should not be imputed\n",
    "train_impute_features_df = train_df.drop(columns=[\n",
    "    \"vehicle_id\", \n",
    "    \"time_step\", \n",
    "    \"length_of_study_time_step\", \n",
    "    \"in_study_repair\", \n",
    "    \"class\"])\n",
    "train_impute_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the mean imputer and fit and apply on training data, and only apply on val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "mean_imputed_train_df = pd.DataFrame(mean_imputer.fit_transform(train_impute_features_df), columns=train_impute_features_df.columns)\n",
    "mean_imputed_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there is no nan values left\n",
    "scroll_df(((mean_imputed_train_df.isna().sum() / len(mean_imputed_train_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the median imputer and fit and apply on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "median_imputed_train_df = pd.DataFrame(median_imputer.fit_transform(train_impute_features_df), columns=train_impute_features_df.columns)\n",
    "median_imputed_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there is no nan values left\n",
    "scroll_df(((mean_imputed_train_df.isna().sum() / len(mean_imputed_train_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the mean and median imputer fitted on training to val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_impute_features_df = val_df.drop(columns=[\n",
    "    \"vehicle_id\", \n",
    "    \"time_step\", \n",
    "    \"length_of_study_time_step\", \n",
    "    \"in_study_repair\", \n",
    "    \"class\"])\n",
    "\n",
    "test_impute_features_df = test_df.drop(columns=[\n",
    "    \"vehicle_id\", \n",
    "    \"time_step\", \n",
    "    \"length_of_study_time_step\", \n",
    "    \"in_study_repair\", \n",
    "    \"class\"])\n",
    "\n",
    "# Val \n",
    "mean_imputed_val_df = pd.DataFrame(mean_imputer.transform(val_impute_features_df), columns=val_impute_features_df.columns)\n",
    "median_imputed_val_df = pd.DataFrame(median_imputer.transform(val_impute_features_df), columns=val_impute_features_df.columns)\n",
    "\n",
    "# Test\n",
    "mean_imputed_test_df = pd.DataFrame(mean_imputer.transform(test_impute_features_df), columns=test_impute_features_df.columns)\n",
    "median_imputed_test_df = pd.DataFrame(median_imputer.transform(test_impute_features_df), columns=test_impute_features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_df(((mean_imputed_val_df.isna().sum() / len(mean_imputed_val_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_df(((median_imputed_val_df.isna().sum() / len(median_imputed_val_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_df(((mean_imputed_test_df.isna().sum() / len(mean_imputed_test_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_df(((median_imputed_test_df.isna().sum() / len(median_imputed_test_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the data with z-score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_1 = StandardScaler()\n",
    "# Standardize the data to mean = 0 and variance = 1\n",
    "dataset_1_train_df = pd.DataFrame(scaler_1.fit_transform(mean_imputed_train_df), columns=mean_imputed_train_df.columns)\n",
    "dataset_1_val_df = pd.DataFrame(scaler_1.transform(mean_imputed_val_df), columns=mean_imputed_val_df.columns)\n",
    "dataset_1_test_df = pd.DataFrame(scaler_1.transform(mean_imputed_test_df), columns=mean_imputed_test_df.columns)\n",
    "\n",
    "print(dataset_1_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_2 = StandardScaler()\n",
    "# Standardize the data to mean = 0 and variance = 1\n",
    "dataset_2_train_df = pd.DataFrame(scaler_2.fit_transform(median_imputed_train_df), columns=median_imputed_train_df.columns)\n",
    "dataset_2_val_df = pd.DataFrame(scaler_2.transform(median_imputed_val_df), columns=median_imputed_val_df.columns)\n",
    "dataset_2_test_df = pd.DataFrame(scaler_2.transform(median_imputed_test_df), columns=median_imputed_test_df.columns)\n",
    "\n",
    "print(dataset_2_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add back the necessary non value columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns_to_add_back = train_df[[\"vehicle_id\", \"time_step\", \"class\"]]\n",
    "val_columns_to_add_back = val_df[[\"vehicle_id\", \"time_step\", \"class\"]]\n",
    "test_columns_to_add_back = test_df[[\"vehicle_id\", \"time_step\", \"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_train_df = pd.concat([train_columns_to_add_back, dataset_1_train_df], axis=1)\n",
    "print(dataset_1_train_df.shape)\n",
    "print(dataset_1_train_df.head())\n",
    "# Check so that index were added back correctly\n",
    "scroll_df(((dataset_1_train_df.isna().sum() / len(dataset_1_train_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2_train_df = pd.concat([train_columns_to_add_back, dataset_2_train_df], axis=1)\n",
    "print(dataset_2_train_df.shape)\n",
    "print(dataset_2_train_df.head())\n",
    "# Check so that index were added back correctly\n",
    "scroll_df(((dataset_2_train_df.isna().sum() / len(dataset_2_train_df)) * 100).to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_val_df = pd.concat([val_columns_to_add_back, dataset_1_val_df], axis=1)\n",
    "print(dataset_1_val_df.shape)\n",
    "print(dataset_1_val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2_val_df = pd.concat([val_columns_to_add_back, dataset_2_val_df], axis=1)\n",
    "print(dataset_2_val_df.shape)\n",
    "print(dataset_2_val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_test_df = pd.concat([test_columns_to_add_back, dataset_1_test_df], axis=1)\n",
    "print(dataset_1_test_df.shape)\n",
    "print(dataset_1_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2_test_df = pd.concat([test_columns_to_add_back, dataset_2_test_df], axis=1)\n",
    "print(dataset_2_test_df.shape)\n",
    "print(dataset_2_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save datasets to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_train_df.to_csv(\"datasets/train_dataset_1.csv\", index=False)\n",
    "dataset_2_train_df.to_csv(\"datasets/train_dataset_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_val_df.to_csv(\"datasets/val_dataset_1.csv\", index=False)\n",
    "dataset_2_val_df.to_csv(\"datasets/val_dataset_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_test_df.to_csv(\"datasets/test_dataset_1.csv\", index=False)\n",
    "dataset_2_test_df.to_csv(\"datasets/test_dataset_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
